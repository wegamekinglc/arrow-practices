{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:01:16.742658Z",
     "start_time": "2025-11-12T16:01:15.565014Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import functools\n",
    "import sys\n",
    "print(sys.version)\n",
    "from timeit import Timer\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import polars as pl\n",
    "import duckdb\n",
    "from datafusion import SessionContext\n",
    "\n",
    "ctx = SessionContext()\n",
    "con = duckdb.connect()\n",
    "\n",
    "\n",
    "def generate_data(number_of_rows):\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    return {\n",
    "        \"order_id\": range(1, number_of_rows + 1),\n",
    "        \"region\": rng.choice(\n",
    "            [\"North\", \"South\", \"East\", \"West\"], size=number_of_rows\n",
    "        ),\n",
    "        \"sales_person\": rng.choice(\n",
    "            [\"Armstrong\", \"Aldrin\", \"Collins\"], size=number_of_rows\n",
    "        ),\n",
    "        \"product\": rng.choice(\n",
    "            [\"Helmet\", \"Oxygen\", \"Boots\", \"Gloves\"], size=number_of_rows\n",
    "        ),\n",
    "        \"sales_income\":  rng.integers(1, 5001, size=number_of_rows),\n",
    "    }"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "768ff6d457aefecc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:01:16.754712Z",
     "start_time": "2025-11-12T16:01:16.750954Z"
    }
   },
   "source": [
    "def create_pandas_dataframe(data_file):\n",
    "    return pd.read_parquet(data_file)\n",
    "\n",
    "def create_pandas_dataframe_with_pyarrow(data_file):\n",
    "    return pq.read_table(data_file).to_pandas(types_mapper=pd.ArrowDtype)\n",
    "\n",
    "def create_pyarrow_dataframe(data_file):\n",
    "    return pq.read_table(data_file)\n",
    "\n",
    "def create_polars_dataframe(data_file):\n",
    "    return pl.read_parquet(data_file)\n",
    "\n",
    "def create_polars_lazyframe(data_file):\n",
    "    return pl.scan_parquet(data_file)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "912db7c25042a0f8",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-12T16:01:16.760545Z"
    }
   },
   "source": [
    "def analyze_pandas_dataframe(pandas_df):\n",
    "    return pandas_df.groupby([\"region\", \"product\", \"sales_person\"])[\n",
    "        \"sales_income\"\n",
    "    ].sum()\n",
    "\n",
    "def analyze_pyarrow_dataframe(pyarrow_df):\n",
    "    return pyarrow_df.group_by([\"region\", \"product\", \"sales_person\"]).aggregate(\n",
    "        [(\"sales_income\", \"sum\")]\n",
    "    )\n",
    "\n",
    "def analyze_datafusion_dataframe():\n",
    "    sql = \"\"\"\n",
    "        SELECT region, product, sales_person, SUM(sales_income) AS total_sales\n",
    "        FROM t\n",
    "        GROUP BY region, product, sales_person\n",
    "        \"\"\"\n",
    "    return ctx.sql(sql).to_arrow_table()\n",
    "\n",
    "def analyze_datafusion_parquet():\n",
    "    sql = \"\"\"\n",
    "        SELECT region, product, sales_person, SUM(sales_income) AS total_sales\n",
    "        FROM t2\n",
    "        GROUP BY region, product, sales_person\n",
    "        \"\"\"\n",
    "    return ctx.sql(sql).to_arrow_table()\n",
    "\n",
    "def analyze_duckdb_dataframe():\n",
    "    sql = \"\"\"\n",
    "        SELECT region, product, sales_person, SUM(sales_income) AS total_sales\n",
    "        FROM t\n",
    "        GROUP BY region, product, sales_person\n",
    "        \"\"\"\n",
    "    return con.execute(sql).fetch_arrow_table()\n",
    "\n",
    "def analyze_duckdb_parquet(data_file):\n",
    "    sql = f\"\"\"\n",
    "        SELECT region, product, sales_person, SUM(sales_income) AS total_sales\n",
    "        FROM \"{data_file}\"\n",
    "        GROUP BY region, product, sales_person\n",
    "        \"\"\"\n",
    "    return con.execute(sql).fetch_arrow_table()\n",
    "\n",
    "\n",
    "def analyze_polars_dataframe(polars_df):\n",
    "    return polars_df.lazy().group_by([\"region\", \"product\", \"sales_person\"]).agg(\n",
    "        total_sales=pl.col(\"sales_income\").sum()\n",
    "    ).collect(engine=\"streaming\")\n",
    "\n",
    "def analyze_polars_lazyframe(polars_lf):\n",
    "    return polars_lf.group_by([\"region\", \"product\", \"sales_person\"]).agg(\n",
    "        total_sales=pl.col(\"sales_income\").sum()\n",
    "    ).collect(engine=\"streaming\")\n",
    "\n",
    "\n",
    "n = 50_000_000\n",
    "\n",
    "data_file = \"data.parquet\"\n",
    "test_data = generate_data(n)\n",
    "table = pa.table(test_data)\n",
    "pq.write_table(table, data_file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9d4af75ea7fd2f33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:00:02.765694Z",
     "start_time": "2025-11-12T16:00:00.521067Z"
    }
   },
   "source": [
    "print(\"Creating DataFrames...\")\n",
    "\n",
    "print(f\"Pandas dataframe creation time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(create_pandas_dataframe, data_file)).timeit(1))\n",
    "print(f\"\\nPandas dataframe with pyarrow creation time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(create_pandas_dataframe_with_pyarrow, data_file)).timeit(1))\n",
    "print(f\"\\nPyarrow dataframe creation time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(create_pyarrow_dataframe, data_file)).timeit(1))\n",
    "print(f\"\\nPolars dataframe creation time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(create_polars_dataframe, data_file)).timeit(1))\n",
    "print(f\"\\nPolars lazyframe creation time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(create_polars_lazyframe, data_file)).timeit(1))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataFrames...\n",
      "Pandas dataframe creation time for 10,000,000 rows:\n",
      "1.4561255999142304\n",
      "\n",
      "Pandas dataframe with pyarrow creation time for 10,000,000 rows:\n",
      "0.33366909995675087\n",
      "\n",
      "Pyarrow dataframe creation time for 10,000,000 rows:\n",
      "0.24838220002129674\n",
      "\n",
      "Polars dataframe creation time for 10,000,000 rows:\n",
      "0.19922349997796118\n",
      "\n",
      "Polars lazyframe creation time for 10,000,000 rows:\n",
      "8.080003317445517e-05\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "a1b168bb-45b0-4332-aac5-4a0c662731d1",
   "metadata": {},
   "source": [
    "# 1. Data analysis\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "id": "60f3a0cd0a3b03e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:00:09.104083Z",
     "start_time": "2025-11-12T16:00:02.788948Z"
    }
   },
   "source": [
    "print(\"-\" * 50)\n",
    "\n",
    "pandas_df = create_pandas_dataframe(data_file)\n",
    "pandas_df_with_pyarrow = create_pandas_dataframe_with_pyarrow(data_file)\n",
    "pyarrow_df = create_pyarrow_dataframe(data_file)\n",
    "if ctx.table_exist(\"t\"):\n",
    "    ctx.deregister_table(\"t\")\n",
    "ctx.from_arrow(pyarrow_df, \"t\")\n",
    "\n",
    "if ctx.table_exist(\"t2\"):\n",
    "    ctx.deregister_table(\"t2\")\n",
    "ctx.register_parquet(\"t2\", data_file)\n",
    "con.register(\"t\", pyarrow_df)\n",
    "polars_df = create_polars_dataframe(data_file)\n",
    "polars_lf = create_polars_lazyframe(data_file)\n",
    "\n",
    "print(f\"Pandas dataframe analysis time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(analyze_pandas_dataframe, pandas_df)).timeit(1))\n",
    "\n",
    "print(f\"\\nPandas dataframe with pyarrow analysis time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(analyze_pandas_dataframe, pandas_df_with_pyarrow)).timeit(1))\n",
    "\n",
    "print(f\"\\nPyarrow dataframe analysis time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(analyze_pyarrow_dataframe, pyarrow_df)).timeit(1))\n",
    "\n",
    "print(f\"\\nDatafusion dataframe analysis time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(analyze_datafusion_dataframe)).timeit(1))\n",
    "\n",
    "print(f\"\\nDatafusion parquet analysis time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(analyze_datafusion_parquet)).timeit(1))\n",
    "\n",
    "print(f\"\\nDuckdb dataframe analysis time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(analyze_duckdb_dataframe)).timeit(1))\n",
    "\n",
    "print(f\"\\nDuckdb parquet analysis time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(analyze_duckdb_parquet, data_file)).timeit(1))\n",
    "\n",
    "print(f\"\\nPolars dataframe analysis time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(analyze_polars_dataframe, polars_df)).timeit(1))\n",
    "\n",
    "print(f\"\\nPolars lazyframe analysis time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(analyze_polars_lazyframe, polars_lf)).timeit(1))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Pandas dataframe analysis time for 10,000,000 rows:\n",
      "1.9524870000313967\n",
      "\n",
      "Pandas dataframe with pyarrow analysis time for 10,000,000 rows:\n",
      "1.367561100050807\n",
      "\n",
      "Pyarrow dataframe analysis time for 10,000,000 rows:\n",
      "0.042230599909089506\n",
      "\n",
      "Datafusion dataframe analysis time for 10,000,000 rows:\n",
      "0.23641319992020726\n",
      "\n",
      "Datafusion parquet analysis time for 10,000,000 rows:\n",
      "0.1275084000080824\n",
      "\n",
      "Duckdb dataframe analysis time for 10,000,000 rows:\n",
      "0.08285230002366006\n",
      "\n",
      "Duckdb parquet analysis time for 10,000,000 rows:\n",
      "0.059257599990814924\n",
      "\n",
      "Polars dataframe analysis time for 10,000,000 rows:\n",
      "0.1406298999208957\n",
      "\n",
      "Polars lazyframe analysis time for 10,000,000 rows:\n",
      "0.14792229991871864\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "01acd671-e142-43cd-862b-43f0321f15b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T16:32:23.651632Z",
     "start_time": "2025-10-20T16:32:23.649773Z"
    }
   },
   "source": [
    "# 2. Serialization/Deserialization\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "id": "ac37bc2fb56908f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:00:11.233481Z",
     "start_time": "2025-11-12T16:00:09.168639Z"
    }
   },
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "pandas_df = create_pandas_dataframe(data_file)\n",
    "pandas_df_with_pyarrow = create_pandas_dataframe_with_pyarrow(data_file)\n",
    "pyarrow_df = create_pyarrow_dataframe(data_file)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "77dfe9a9-13c6-4b44-b0e2-5516ceabc573",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:00:13.875333Z",
     "start_time": "2025-11-12T16:00:13.870733Z"
    }
   },
   "source": [
    "def serialize_pandas_df(pandas_df):\n",
    "    return pickle.dumps(pandas_df, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def serialize_pyarrow_df(pyarrow_df):\n",
    "    sink = pa.BufferOutputStream()\n",
    "    with pa.ipc.new_file(sink, pyarrow_df.schema) as writer:\n",
    "        writer.write_table(pyarrow_df)\n",
    "\n",
    "    buf = sink.getvalue()\n",
    "    return buf.to_pybytes()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "26cbf4c6-a871-41d7-8f83-02f1eace3629",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:00:22.756972Z",
     "start_time": "2025-11-12T16:00:20.463419Z"
    }
   },
   "source": [
    "print(f\"Pandas dataframe serialize time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(serialize_pandas_df, pandas_df)).timeit(1))\n",
    "\n",
    "print(f\"\\nPandas dataframe with pyarrow serialize time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(serialize_pandas_df, pandas_df_with_pyarrow)).timeit(1))\n",
    "\n",
    "print(f\"\\nPyarrow dataframe serialize time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(serialize_pyarrow_df, pyarrow_df)).timeit(1))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas dataframe serialize time for 10,000,000 rows:\n",
      "0.9356666000094265\n",
      "\n",
      "Pandas dataframe with pyarrow serialize time for 10,000,000 rows:\n",
      "0.6746197000611573\n",
      "\n",
      "Pyarrow dataframe serialize time for 10,000,000 rows:\n",
      "0.6774355999659747\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "85bfc562-14e1-479a-b831-24eb028e5773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:00:26.178224Z",
     "start_time": "2025-11-12T16:00:24.209376Z"
    }
   },
   "source": [
    "pandas_df_bytes = serialize_pandas_df(pandas_df)\n",
    "pandas_df_with_pyarrow_bytes = serialize_pandas_df(pandas_df_with_pyarrow)\n",
    "pyarrow_df_bytes = serialize_pyarrow_df(pyarrow_df)\n",
    "\n",
    "print(f\"pandas bytes              : {len(pandas_df_bytes)}\")\n",
    "print(f\"pandas with pyarrow bytes : {len(pandas_df_with_pyarrow_bytes)}\")\n",
    "print(f\"pyarrow_df_bytes bytes    : {len(pyarrow_df_bytes)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas bytes              : 220069131\n",
      "pandas with pyarrow bytes : 455835403\n",
      "pyarrow_df_bytes bytes    : 455874210\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "f782c956-d0cb-4cf0-bdc9-17bdc4d40f3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:00:26.203255Z",
     "start_time": "2025-11-12T16:00:26.199346Z"
    }
   },
   "source": [
    "def deserialize_pandas_df(pandas_df_bytes):\n",
    "    return pickle.loads(pandas_df_bytes)\n",
    "\n",
    "def deserialize_pyarrow_df(pyarrow_df_bytes):\n",
    "    with pa.ipc.open_file(pyarrow_df_bytes) as reader:\n",
    "        return reader.read_all()"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "7bda8a63-043a-4821-87db-f11169738ff8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:00:28.063783Z",
     "start_time": "2025-11-12T16:00:26.216605Z"
    }
   },
   "source": [
    "print(f\"Pandas dataframe deserialize time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(deserialize_pandas_df, pandas_df_bytes)).timeit(1))\n",
    "print(f\"\\nPandas dataframe with pyarrow deserialize time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(deserialize_pandas_df, pandas_df_with_pyarrow_bytes)).timeit(1))\n",
    "print(f\"\\nPyarrow dataframe deserialize time for {n:,} rows:\")\n",
    "print(Timer(functools.partial(deserialize_pyarrow_df, pyarrow_df_bytes)).timeit(1))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas dataframe deserialize time for 10,000,000 rows:\n",
      "1.672693199943751\n",
      "\n",
      "Pandas dataframe with pyarrow deserialize time for 10,000,000 rows:\n",
      "0.1634343999903649\n",
      "\n",
      "Pyarrow dataframe deserialize time for 10,000,000 rows:\n",
      "0.00563320005312562\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:00:36.900257Z",
     "start_time": "2025-11-12T16:00:36.898174Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "fcb69f25513f4bd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d3a16bf9eb827a25"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
